% ---------- Embedded Bibliography ----------
\begin{filecontents*}[overwrite]{\jobname.bib}
@article{chan2020computer, author={Chan, Heang-Ping and Samala, Ravi K. and Hadjiiski, Lubomir M. and Sahiner, Chaovalit}, title={Computer-aided diagnosis in the era of deep learning}, journal={Medical Physics}, year={2020}, volume={47}, number={5}, pages={e218--e227}, doi={10.1002/mp.13764}, url={https://doi.org/10.1002/mp.13764}}

@article{wallace2022impact, author={Wallace, Michael B. and Sharma, Prateek and Bhandari, Pradeep and East, James and Antonelli, Giulio and Rostom, Alaa and Arsaradnil, Ramesh and Tsiamoulos, Zacharias and Parra-Blanco, Adolfo and Kudo, Seiyuu and Itoh, Hiroshi and Siersema, Peter and Wilson, Ana and Misawa, Masashi and Mori, Yuichi and Hassan, Cesare}, title={Impact of Artificial Intelligence on Miss Rate of Colorectal Neoplasia}, journal={Gastroenterology}, year={2022}, volume={163}, number={1}, pages={295--304}, doi={10.1053/j.gastro.2022.03.007}, url={https://doi.org/10.1053/j.gastro.2022.03.007}}

@article{chan2009fewer, author={Chan, Michael Y. and Cohen, Hartley and Spiegel, Brennan M.R.}, title={Fewer polyps detected by colonoscopy as the day progresses at a Veteranâ€™s Administration teaching hospital}, journal={Clinical Gastroenterology and Hepatology}, year={2009}, volume={7}, number={11}, pages={1217--1223}, doi={10.1016/j.cgh.2009.07.013}, url={https://doi.org/10.1016/j.cgh.2009.07.013}}

@article{kaminski2010quality, author={Kaminski, Michal F. and Regula, Jaroslaw and Kraszewska, Ewa and Polkowski, Marcin and Wojciechowska, Urszula and Didkowska, Joanna and Zwierko, Maria and Rupinski, Maciej and Nowacki, Marek P. and Butruk, Eugeniusz}, title={Quality indicators for colonoscopy and the risk of interval cancer}, journal={New England Journal of Medicine}, year={2010}, volume={362}, number={19}, pages={1795--1803}, doi={10.1056/NEJMoa0907667}, url={https://doi.org/10.1056/NEJMoa0907667}}

@misc{wang2023endofm, author={Wang, Yutong and Zhang, Zixuan and Liu, Huahui and Ding, Xinlong and Hu, Jie and Chen, Hao and Wu, Jianfei}, title={Endo-FM: Foundation Model for Endoscopy Video Analysis via Large-scale Pre-training}, howpublished={arXiv preprint arXiv:2306.16741}, year={2023}, url={https://arxiv.org/abs/2306.16741}}

@misc{liu2025endodino, author={Liu, Yuncheng and Wang, Yutong and Zhang, Zixuan and Chen, Hao and Wu, Jianfei}, title={Endo-DINO: Self-Supervised Pre-training for Endoscopy with Vision Transformers}, howpublished={arXiv preprint arXiv:2501.05488}, year={2025}, url={https://arxiv.org/abs/2501.05488}}

@inproceedings{wang2024endovla, author={Wang, Rui and Hong, Yixiang and He, Yuting and Wu, Jun and Yuan, Wu}, title={EndoVLA: Progressive Endoscopic Vision-Language-Action Model}, booktitle={Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024}, year={2024}, publisher={Springer}, pages={446--456}, doi={10.1007/978-3-031-72083-3_42}, url={https://doi.org/10.1007/978-3-031-72083-3_42}}

@article{shah2002patient, author={Shah, Sandeep G. and Brooker, Julian C. and Thapar, Chetan and Williams, Christopher B. and Saunders, Brian P.}, title={Patient pain during colonoscopy: an analysis using real-time magnetic endoscope imaging}, journal={Endoscopy}, year={2002}, volume={34}, number={6}, pages={435--440}, doi={10.1055/s-2002-32052}, url={https://doi.org/10.1055/s-2002-32052}}

@article{martin2020enabling, author={Martin, James W. and Scaglioni, Bruno and Norton, Joseph C. and Subramanian, Venkataraman and Arezzo, Alberto and Obstein, Keith L. and Valdastri, Pietro}, title={Enabling the future of colonoscopy with intelligent and autonomous magnetic manipulation}, journal={Nature Machine Intelligence}, year={2020}, volume={2}, number={10}, pages={595--602}, doi={10.1038/s42256-020-00231-9}, url={https://doi.org/10.1038/s42256-020-00231-9}}

@article{wang2022millimeter, author={Wang, Zhanfeng and Shi, Kaige and Xu, Qingsong}, title={A Millimeter-Scale Magnetic Soft Robot With Spider-Inspired Multi-Modal Locomotion and Object Manipulation}, journal={IEEE/ASME Transactions on Mechatronics}, year={2022}, volume={27}, number={6}, pages={4676--4687}, doi={10.1109/TMECH.2022.3164478}, url={https://doi.org/10.1109/TMECH.2022.3164478}}

@article{hong2021autonomous, author={Hong, Yixiang and Zhang, Yan and Meng, Max Q.-H.}, title={Autonomous pylorus traversal in magnetic capsule endoscopy via deep reinforcement learning}, journal={IEEE Transactions on Robotics}, year={2021}, volume={37}, number={4}, pages={1094--1108}, doi={10.1109/TRO.2020.3047717}, url={https://doi.org/10.1109/TRO.2020.3047717}}

@misc{wang2024feasibility, author={Wang, Rui and Hong, Yixiang and He, Yuting and Wu, Jun and Yuan, Wu}, title={A Feasibility Study on Humanoid Robot Teleoperation for Laparoscopic Surgery}, howpublished={arXiv preprint}, year={2024}}

@article{piazza2019century, author={Piazza, Cristina and Grioli, Giorgio and Catalano, Manuel G. and Bicchi, Antonio}, title={A Century of Robotic Hands}, journal={Annual Review of Control, Robotics, and Autonomous Systems}, year={2019}, volume={2}, number={1}, pages={1--32}, doi={10.1146/annurev-control-060117-105003}, url={https://doi.org/10.1146/annurev-control-060117-105003}}

@inproceedings{hong2023safe, author={Hong, Yixiang and Wang, Rui and He, Yuting and Wu, Jun and Yuan, Wu}, title={Safe Reinforcement Learning for Autonomous Flexible Endoscopy}, booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, year={2023}, pages={9460--9466}, doi={10.1109/ICRA48891.2023.10161476}, url={https://doi.org/10.1109/ICRA48891.2023.10161476}}

@article{crockett2022screening45, author={Crockett, Seth D. and Ladabaum, Uri}, title={Potential Effects of Lowering Colorectal Cancer Screening Age to 45 Years on Colonoscopy Demand, Case Mix, and Adenoma Detection Rate}, journal={Gastroenterology}, year={2022}, volume={162}, number={3}, pages={984--986.e5}, doi={10.1053/j.gastro.2021.11.024}, url={https://doi.org/10.1053/j.gastro.2021.11.024}}

@article{topol2019high, author={Topol, Eric J.}, title={High-performance medicine: the convergence of human and artificial intelligence}, journal={Nature Medicine}, year={2019}, volume={25}, number={1}, pages={44--56}, doi={10.1038/s41591-018-0300-7}, url={https://doi.org/10.1038/s41591-018-0300-7}}
\end{filecontents*}

\documentclass[12pt]{article}

% ---------- Packages ----------
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[strict]{csquotes}
\usepackage{mathptmx}  % Times Roman font (NEJM requirement)
\usepackage{setspace}
\usepackage{authblk}
\usepackage{amsmath, amssymb}
\usepackage{siunitx}
\usepackage{threeparttable}
\usepackage{longtable,threeparttablex,booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
\usepackage[backend=biber,style=nejm,sorting=none,maxnames=6,minnames=3,terseinits=true,isbn=false]{biblatex}
\addbibresource{\jobname.bib}
\captionsetup{font=small, labelfont=bf}
\usepackage{tabularx}
\usepackage{lineno}

% ---------- Formatting tweaks ----------
\setlength{\parskip}{0.6em}
\setlength{\parindent}{0pt}
\doublespacing
\linenumbers

% ---------- Title & Authors ----------
\title{BEYOND THE SCOPE: WILL HUMANOID ROBOTS DEFINE THE FUTURE OF ENDOSCOPY?}

\author[1]{\textbf{Yichen Wang}}
\author[1]{\textbf{Yuting Huang}}
\author[2]{\textbf{Mario El Hayek}}
\author[1]{\textbf{Vivek Kumbhari}}
\author[1]{\textbf{Michael B. Wallace}}

\affil[1]{Division of Gastroenterology and Hepatology, Mayo Clinic, Jacksonville, FL, USA}
\affil[2]{Department of Medicine, Mayo Clinic, Jacksonville, FL, USA}

\date{}

% ---------- Document ----------
\begin{document}
\maketitle

% Correspondence
\section*{Correspondence}
Michael B. Wallace, MD, MPH, Mayo Clinic, 4500 San Pablo Rd, Jacksonville, FL 32224

For the past half-century, the technological trajectory of gastrointestinal endoscopy has been focused on visualization. Innovations in optics and high-definition sensors were designed to present a clearer picture to the human eye, placing the task of interpretation solely on the physician. Recent years have seen the integration of artificial intelligence (AI), in the form of computer-aided detection (CADe) and diagnosis (CADx). \supercite{chan2020computer} Systems such as GI Genius have demonstrated the ability to reduce adenoma miss rates by acting as a "second observer" \supercite{wallace2022impact} However, these advancements remain fundamentally passive; they reside on the screen, disconnected from the physical activity of the procedure. The endoscopist still bears the total cognitive and physical load of navigating the tortuous anatomy of the colon, creating a bottleneck where fatigue and variable motor skills contribute to significant inter-observer variability in procedural quality \supercite{chan2009fewer,kaminski2010quality}.

The discipline is now standing at the precipice of the epoch of embodied AI. This new paradigm represents the integration of advanced cognitive architectures, specifically vision-language-action (VLA) models, with physical robotic actuation. It marks the transition from systems that merely "see" to systems that "act." This shift is not merely incremental; it is a fundamental reimagining of what an endoscopic system can be. Embodied AI envisions a robotic agent capable of perceiving the complex, deformable environment of the gut, reasoning about anatomical landmarks in semantic terms, and autonomously executing kinematic maneuvers. This review analyzes the theoretical underpinnings of this convergence, exploring the specialized hardware of soft robotics and the emerging, disruptive potential of humanoid robots as autonomous operators.

The brain of an embodied agent requires capabilities far exceeding old convolutional neural networks. The field is adopting foundational vision-language models, to bridge medical knowledge and vision signals. Architectures like Endo-FM \supercite{wang2023endofm} and Endo-DINO \supercite{liu2025endodino} provide pre-trained foundational models for downstream tasks. The most significant leap is the Endoscopic Vision-Language-Action (EndoVLA) model. \supercite{wang2024endovla} EndoVLA is an end-to-end framework where a live video feed and a human text prompt generate direct control commands. This architecture solves the "semantic gap" in robotics by translating high-level intent, such as "resect that polyp," into low-level motor signals. To domesticate general-purpose VLMs for this safety-critical task, researchers employ a Dual-Phase Fine-Tuning strategy. \supercite{wang2024endovla} The model first undergoes Supervised Fine-Tuning to learn the "grammar" of robotic kinematics, followed by Reinforcement Fine-Tuning to hone precision through interaction.

A sophisticated cognitive engine is futile without a physical body capable of safe execution. One dominant approach to this challenge is the redesign of the instrument itself. Traditional semi-rigid endoscopes are prone to causing painful looping; consequently, the hardware for autonomous GI endoscopy is evolving toward soft robotics, prioritizing compliance and safety. \supercite{shah2002patient,martin2020enabling} A paradigm-shifting example is the spider-inspired magnetic soft robot developed at the University of Macau \supercite{wang2022millimeter}. Modeled after the golden wheel spider, this device utilizes a magneto-active elastomer matrix to switch between rolling and climbing locomotion modes. Unlike passive capsules, this robot can actively climb over haustral folds and navigate upside down via magnetic anchoring. Simultaneously, Magnetic Capsule Endoscopy is maturing from teleoperated systems to autonomous platforms. Projects like MAGIC-AIM integrate VLM architectures with magnetic guidance, allowing the capsule to identify the pylorus and autonomously execute the traversal maneuver \supercite{wang2024endovla,hong2021autonomous}. This automation ensures a complete map of the gastric mucosa is generated regardless of operator fatigue.

While soft robotics reinvent the tool, a parallel lineage proposes replacing the operator. The rise of general-purpose humanoid robots offers a tantalizing alternative: an autonomous agent that walks into the endoscopy suite, picks up the existing standard colonoscope, and performs the procedure using the same ergonomics designed for human hands.

Backward compatibility drives this approach. Hospitals worldwide have billions of dollars invested in standard flexible endoscopes. Soft robotic solutions require abandoning this infrastructure for expensive, proprietary magnetic guidance systems. A humanoid robot, conversely, is a "drop-in" replacement. It utilizes the current capital equipment, navigating the colonoscope via the control wheels and shaft torque just as a human fellow would. Recent feasibility studies have demonstrated that humanoids can effectively manipulate laparoscopic tools via teleoperation, suggesting that the transition to flexible endoscopy is a matter of software refinement rather than hardware impossibility \supercite{wang2024feasibility}.

The cold hard truth is we are stalled by clinical and technical inertia, primarily the "Dexterity Gap." We have spent decades refining the endoscope as a tool for humans, yet we now face a reality where our current hardware is a legacy anchor. The industry is hooked on high-margin, semi-disposable scopes that are fundamentally incompatible with the precision of high-torque robotic motors \supercite{martin2020enabling} If we continue to ignore the friction between 20th-century ergonomics and 21st-century AI, we risk creating a perpetual pilot phase that never reaches the patient's bedside. Hospitals are loath to cannibalize their existing infrastructure, yet they complain about a shortage of hands to hold the scope. The human hand is a marvel of compliance and sensory feedback, capable of sensing the subtle resistance of a loop forming in the sigmoid colon through the shaft of the scope. Current commercial humanoid hands are often rigid, under-actuated grippers designed for gross manipulation, not the delicate interplay of torque and translation required in endoscopy \supercite{piazza2019century}. For a humanoid to be viable, it requires reliable, dexterous hands equipped with high-fidelity tactile sensors at a low price point. The robot must be able to manipulate the angulation wheels with its thumb while simultaneously torquing the shaft with its right hand, a bimanual coordination task that remains an open challenge in robotics control.

Furthermore, there is a critical scarcity of training data known as the "proprioceptive void." While we have millions of hours of endoscopic video, we have virtually zero data recording the actions of the endoscopist's hands synchronized with that video \supercite{wallace2022impact,martin2020enabling}. We know what the colon looks like, but we do not know what the doctor's hands were doing to produce that view. To train a humanoid EndoVLA, we must bridge this gap by instrumenting expert endoscopists with motion-capture gloves or sensors during live procedures, creating massive "Vision-Action" datasets. Once this data exists, the potential for scaling is boundless. A model trained on the collective motor skills of the world's best endoscopists could be deployed to a humanoid in a rural clinic, instantly granting it the dexterity of a master clinician.

The humanoid approach warrants optimism. General robotics outpaces specialized medical tools due to massive tech investment. As costs fall, humanoids will address the global gastroenterologist shortage. A future where robots perform screening colonoscopies 24/7 is a converging probability. Economic incentives are powerful. As reimbursement fluctuates, a robotic workforce becomes an existential necessity. Continuous operation eliminates the 'off-hours' penalty where outcomes suffer due to staffing. A robot does not sleep, and its hand never shakes at the end of a shift. This consistency is the true promise of the revolution.

The integration of the brain and body occurs in the navigation loop. FM and VLM architectures provide the reasoning required for autonomous tracking through language-conditioned tasks. By processing a prompt and video feed to output discrete motion commands, these systems offer Zero-Shot Generalization, allowing the tracking of novel objects like surgical clips without retraining \supercite{wang2024endovla}.

Generative AI necessitates rigorous safety. Researchers are employing Safe Reinforcement Learning using Control Barrier Functions that act as mathematical virtual walls, analyzing actions against safety constraints to prevent perforation. \supercite{hong2023safe} To bridge the Sim-to-Real gap, agents are trained in high-fidelity simulators using domain randomization, ensuring robust performance despite the chaotic textures of a living patient.

The convergence of embodied AI and GI endoscopy signifies a fundamental restructuring of gastroenterological care. The technological pillars are in place: Vision-Language Models have solved the semantic gap, enabling robots to understand medical intent; and two distinct physical lineages are vying for dominance. On one hand, soft robotics offers a safety-first reinvention of the instrument; on the other, humanoid robotics offers a scalable, backward-compatible reinvention of the operator. As we look toward the next decade, the standard of care is poised to shift from manual operation to supervisory control. Future screening protocols may involve patients being treated by a humanoid agent that wields standard instruments with the precision of a machine and the adaptability of a human. This evolution promises to democratize high-quality care, reducing the physical burden on physicians and ensuring that every patient receives a procedure defined by the collective intelligence of the field rather than the variability of a single operator. The era of the passive tool is ending; the era of the embodied partner has begun.

For practicing endoscopists, a "robot-ready room" can log wheel rotations, torque, and finger kinematics during routine lists to generate vision--action corpora without slowing workflow. Health systems absorbing the screening-age drop to 45 years could designate supervised data-collection blocks and trial overnight humanoid-run screening lines with remote oversight to clear backlogs \supercite{crockett2022screening45}. These experiments demand audit trails, explainable policies, and governance that preserve human accountability even as robots offload repetitive motor work \supercite{topol2019high}.

\noindent\textbf{Keywords:} Artificial Intelligence, Humanoid Robots, Soft Robotics, Vision-Language Models

\section*{ABBREVIATIONS}
AI, Artificial Intelligence; CADe, Computer-Aided Detection; CADx, Computer-Aided Diagnosis; EndoVLA, Endoscopic Vision-Language-Action; FM, Foundation Model; GI, Gastrointestinal; VLM, Vision-Language Model.
\section*{References}
\printbibliography

% ---------- End ----------
\end{document}
